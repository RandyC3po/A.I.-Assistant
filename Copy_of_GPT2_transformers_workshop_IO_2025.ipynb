{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RandyC3po/A.I.-Assistant/blob/main/Copy_of_GPT2_transformers_workshop_IO_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "# Pretrain the GPT2 model\n",
        "\n",
        "This notebook seeks to replicate Andrey Karpathy's highly popular [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) project and trains a GPT2 model from scratch using JAX+TPU and the [OpenWebText dataset](https://huggingface.co/datasets/Skylion007/openwebtext).\n",
        "\n",
        "You can run this on Colab, Kaggle or GCP TPUs, although Colab TPU v2 can only run the smallest 124M GPT model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD3bo9FxhrTE"
      },
      "source": [
        "## Determine platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "m1ca1Pt3A1pJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Registration State",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Violation Description",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "count",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "a938336b-19b0-4e23-8f7e-752d3ef42d06",
              "rows": [
                [
                  "0",
                  "CA",
                  "Double Parking",
                  "50304"
                ],
                [
                  "1",
                  "NJ",
                  "Expired Meter",
                  "50150"
                ],
                [
                  "2",
                  "NY",
                  "Expired Meter",
                  "50197"
                ],
                [
                  "3",
                  "TX",
                  "Expired Meter",
                  "49994"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 4
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Registration State</th>\n",
              "      <th>Violation Description</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CA</td>\n",
              "      <td>Double Parking</td>\n",
              "      <td>50304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NJ</td>\n",
              "      <td>Expired Meter</td>\n",
              "      <td>50150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NY</td>\n",
              "      <td>Expired Meter</td>\n",
              "      <td>50197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TX</td>\n",
              "      <td>Expired Meter</td>\n",
              "      <td>49994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Registration State Violation Description  count\n",
              "0                 CA        Double Parking  50304\n",
              "1                 NJ         Expired Meter  50150\n",
              "2                 NY         Expired Meter  50197\n",
              "3                 TX         Expired Meter  49994"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "\t\t\t  \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "\t\"Registration State\": np.random.choice(states, size=num_rows),\n",
        "\t\"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "\t\"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "\t\"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "\t\"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows, dtype=np.int64)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MuFEBtNkuAdT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Platform is GCP\n"
          ]
        }
      ],
      "source": [
        "# Assume using Cloud TPU by default\n",
        "platform = \"GCP\"\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  platform = \"Colab\"\n",
        "except ImportError:\n",
        "  pass # not colab\n",
        "\n",
        "try:\n",
        "  import kaggle\n",
        "  platform = \"Kaggle\"\n",
        "except (ImportError, IOError):\n",
        "  pass # not kaggle\n",
        "\n",
        "print(f\"Platform is {platform}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci1qnrl5-aNN"
      },
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kSdAJZYc-aNN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [67 lines of output]\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl against tag cp310-cp310-manylinux_2_24_aarch64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp310-cp310-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl against tag cp310-cp310-manylinux_2_28_aarch64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl against tag cp310-cp310-manylinux_2_24_x86_64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl against tag cp310-cp310-manylinux_2_28_x86_64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp311-cp311-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl against tag cp311-cp311-manylinux_2_24_aarch64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp311-cp311-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl against tag cp311-cp311-manylinux_2_28_aarch64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl against tag cp311-cp311-manylinux_2_24_x86_64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl against tag cp311-cp311-manylinux_2_28_x86_64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl against tag cp312-cp312-manylinux_2_28_aarch64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp312-cp312-manylinux_2_24_aarch64.manylinux_2_28_aarch64.whl against tag cp312-cp312-manylinux_2_24_aarch64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl against tag cp312-cp312-manylinux_2_24_x86_64\n",
            "      INFO:wheel-stub:Testing wheel cudf_cu11-25.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl against tag cp312-cp312-manylinux_2_28_x86_64\n",
            "        File \"C:\\Users\\Randy Cecil\\AppData\\Local\\Temp\\pip-build-env-gwti9zu4\\overlay\\Lib\\site-packages\\wheel_stub\\wheel.py\", line 249, in download_wheel\n",
            "          return download_manual(wheel_directory, distribution, version, config)\n",
            "        File \"C:\\Users\\Randy Cecil\\AppData\\Local\\Temp\\pip-build-env-gwti9zu4\\overlay\\Lib\\site-packages\\wheel_stub\\wheel.py\", line 185, in download_manual\n",
            "          raise RuntimeError(f\"Didn't find wheel for {distribution} {version}\")\n",
            "      Traceback (most recent call last):\n",
            "        File \u001b[35m\"C:\\Users\\Randy Cecil\\AppData\\Local\\Temp\\pip-build-env-gwti9zu4\\overlay\\Lib\\site-packages\\wheel_stub\\wheel.py\"\u001b[0m, line \u001b[35m249\u001b[0m, in \u001b[35mdownload_wheel\u001b[0m\n",
            "          return download_manual(wheel_directory, distribution, version, config)\n",
            "        File \u001b[35m\"C:\\Users\\Randy Cecil\\AppData\\Local\\Temp\\pip-build-env-gwti9zu4\\overlay\\Lib\\site-packages\\wheel_stub\\wheel.py\"\u001b[0m, line \u001b[35m185\u001b[0m, in \u001b[35mdownload_manual\u001b[0m\n",
            "          raise RuntimeError(f\"Didn't find wheel for {distribution} {version}\")\n",
            "      \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mDidn't find wheel for cudf-cu11 25.4.0\u001b[0m\n",
            "      \n",
            "      During handling of the above exception, another exception occurred:\n",
            "      \n",
            "      Traceback (most recent call last):\n",
            "        File \u001b[35m\"c:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "        File \u001b[35m\"c:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
            "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"c:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m178\u001b[0m, in \u001b[35mprepare_metadata_for_build_wheel\u001b[0m\n",
            "          whl_basename = backend.build_wheel(metadata_directory, config_settings)\n",
            "        File \u001b[35m\"C:\\Users\\Randy Cecil\\AppData\\Local\\Temp\\pip-build-env-gwti9zu4\\overlay\\Lib\\site-packages\\wheel_stub\\buildapi.py\"\u001b[0m, line \u001b[35m29\u001b[0m, in \u001b[35mbuild_wheel\u001b[0m\n",
            "          return download_wheel(pathlib.Path(wheel_directory), config_settings)\n",
            "        File \u001b[35m\"C:\\Users\\Randy Cecil\\AppData\\Local\\Temp\\pip-build-env-gwti9zu4\\overlay\\Lib\\site-packages\\wheel_stub\\wheel.py\"\u001b[0m, line \u001b[35m251\u001b[0m, in \u001b[35mdownload_wheel\u001b[0m\n",
            "          \u001b[31mreport_install_failure\u001b[0m\u001b[1;31m(distribution, version, config, exception_context)\u001b[0m\n",
            "          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\Randy Cecil\\AppData\\Local\\Temp\\pip-build-env-gwti9zu4\\overlay\\Lib\\site-packages\\wheel_stub\\error.py\"\u001b[0m, line \u001b[35m67\u001b[0m, in \u001b[35mreport_install_failure\u001b[0m\n",
            "          raise InstallFailedError(\n",
            "          ...<9 lines>...\n",
            "          )\n",
            "      \u001b[1;35mwheel_stub.error.InstallFailedError\u001b[0m: \u001b[35m\n",
            "      *******************************************************************************\n",
            "      \n",
            "      The installation of cudf-cu11 for version 25.4.0 failed.\n",
            "      \n",
            "      This is a special placeholder package which downloads a real wheel package\n",
            "      from https://pypi.nvidia.com/. If https://pypi.nvidia.com/ is not reachable, we\n",
            "      cannot download the real wheel file to install.\n",
            "      \n",
            "      You might try installing this package via\n",
            "      ```\n",
            "      $ pip install --extra-index-url https://pypi.nvidia.com/ cudf-cu11\n",
            "      ```\n",
            "      \n",
            "      Here is some debug information about your platform to include in any bug\n",
            "      report:\n",
            "      \n",
            "      Python Version: CPython 3.13.3\n",
            "      Operating System: Windows 11\n",
            "      CPU Architecture: AMD64\n",
            "      nvidia-smi command not found. Ensure NVIDIA drivers are installed.\n",
            "      \n",
            "      *******************************************************************************\n",
            "      \u001b[0m\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting cudf-cu11\n",
            "  Using cached cudf_cu11-25.4.0.tar.gz (2.7 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Registration State",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Violation Description",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "count",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "e7cdf00f-680e-4e74-a5d9-a180d6ea6a1c",
              "rows": [
                [
                  "0",
                  "CA",
                  "No Parking",
                  "50542"
                ],
                [
                  "1",
                  "NJ",
                  "Expired Meter",
                  "50056"
                ],
                [
                  "2",
                  "NY",
                  "Fire Hydrant",
                  "50170"
                ],
                [
                  "3",
                  "TX",
                  "No Parking",
                  "50227"
                ]
              ],
              "shape": {
                "columns": 3,
                "rows": 4
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Registration State</th>\n",
              "      <th>Violation Description</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CA</td>\n",
              "      <td>No Parking</td>\n",
              "      <td>50542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NJ</td>\n",
              "      <td>Expired Meter</td>\n",
              "      <td>50056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NY</td>\n",
              "      <td>Fire Hydrant</td>\n",
              "      <td>50170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TX</td>\n",
              "      <td>No Parking</td>\n",
              "      <td>50227</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Registration State Violation Description  count\n",
              "0                 CA            No Parking  50542\n",
              "1                 NJ         Expired Meter  50056\n",
              "2                 NY          Fire Hydrant  50170\n",
              "3                 TX            No Parking  50227"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows, dtype=np.int64)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3TIVzZ34AYXZ"
      },
      "outputs": [],
      "source": [
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows, dtype=np.int64)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "XvqzQOUxAg7L"
      },
      "outputs": [],
      "source": [
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows, dtype=np.int64)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-oadnptAce0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "t6LhLpFIAZL0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install cudf-cu11 --extra-index-url=https://pypi.nvidia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:18.414249Z",
          "iopub.status.busy": "2025-02-21T04:30:18.414058Z",
          "iopub.status.idle": "2025-02-21T04:30:51.718249Z",
          "shell.execute_reply": "2025-02-21T04:30:51.716902Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.414229Z"
        },
        "id": "6zMsOIc7ouCO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q jax-ai-stack\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cWxBvz6bZDd"
      },
      "source": [
        "Confirm we have TPUs set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:51.719450Z",
          "iopub.status.busy": "2025-02-21T04:30:51.719166Z",
          "iopub.status.idle": "2025-02-21T04:30:59.572796Z",
          "shell.execute_reply": "2025-02-21T04:30:59.571540Z",
          "shell.execute_reply.started": "2025-02-21T04:30:51.719422Z"
        },
        "id": "uZUaKdi5bSEN",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[CpuDevice(id=0)]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:59.573940Z",
          "iopub.status.busy": "2025-02-21T04:30:59.573640Z",
          "iopub.status.idle": "2025-02-21T04:31:01.600418Z",
          "shell.execute_reply": "2025-02-21T04:31:01.598633Z",
          "shell.execute_reply.started": "2025-02-21T04:30:59.573916Z"
        },
        "id": "MKYFNOhdLq98",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (0.5.3)\n",
            "Requirement already satisfied: jaxlib in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (0.5.3)\n",
            "Requirement already satisfied: flax in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (0.10.5)\n",
            "Requirement already satisfied: optax in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (0.2.4)\n",
            "Requirement already satisfied: orbax in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (0.1.9)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: wandb in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (0.19.11)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from jax) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.25 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from jax) (2.2.6)\n",
            "Requirement already satisfied: opt_einsum in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from jax) (1.15.3)\n",
            "Requirement already satisfied: msgpack in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from flax) (0.11.12)\n",
            "Requirement already satisfied: tensorstore in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from flax) (0.1.75)\n",
            "Requirement already satisfied: rich>=11.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from flax) (14.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from flax) (4.13.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from optax) (2.3.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from optax) (0.1.89)\n",
            "Requirement already satisfied: etils[epy] in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from optax) (1.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (6.31.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: pydantic<3 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from wandb) (80.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from chex>=0.1.87->optax) (1.0.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: nest_asyncio in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: humanize in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from orbax-checkpoint->flax) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from rich>=11.1->flax) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2025.5.1)\n",
            "Requirement already satisfied: importlib_resources in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
            "Requirement already satisfied: zipp in c:\\users\\randy cecil\\onedrive\\desktop\\a.i.-assistant-1\\.venv\\lib\\site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.22.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'distutils'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msharding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mesh, PartitionSpec \u001b[38;5;28;01mas\u001b[39;00m P, NamedSharding\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\__init__.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wb_logging \u001b[38;5;28;01mas\u001b[39;00m _wb_logging\n\u001b[32m     23\u001b[39m _wb_logging.configure_wandb_logger()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m     29\u001b[39m wandb.wandb_lib = wandb_sdk.lib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\__init__.py:25\u001b[39m\n\u001b[32m      3\u001b[39m __all__ = (\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSettings\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhelper\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifacts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwandb_alerts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwandb_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\artifacts\\artifact.py:29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data_types, env, util\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnormalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, RetryingClient, Run\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\data_types.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"This module defines data types for logging rich, interactive visualizations to W&B.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mData types include common media types, like images, audio, and videos,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03mand upload them to the W&B server.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_types\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_types\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_types\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmedia\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchableMedia, Media\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_types\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_types\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwb_value\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WBValue\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\audio.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _dtypes\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_private\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MEDIA_TMP\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_types\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmedia\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchableMedia\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAudio\u001b[39;00m(BatchableMedia):\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wandb class for audio clips.\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33;03m        caption: (string) Caption to display with audio.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\base_types\\media.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m filesystem\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpaths\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogicalPath\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwb_value\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WBValue\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\data_types\\base_types\\wb_value.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, ClassVar, Dict, List, Optional, Type, Union\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_setup\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifacts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifact\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Artifact\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_setup.py:28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_DIR\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m import_hooks, wb_logging\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_settings\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_util, server\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_settings.py:39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UsageError\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wandb_settings_pb2\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apikey, credentials, ipython\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgitlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GitRepo\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_moment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunMoment\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\lib\\apikey.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NETRC_FILES, get_netrc_auth\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InternalApi\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m term\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m url_registry\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\apis\\__init__.py:44\u001b[39m\n\u001b[32m     41\u001b[39m reset_path = util.vendor_setup()\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     46\u001b[39m reset_path()\n\u001b[32m     48\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mInternalApi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPublicApi\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\apis\\public\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api, RetryingClient, requests\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifacts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     ArtifactCollection,\n\u001b[32m      4\u001b[39m     ArtifactCollections,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     RunArtifacts,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpublic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautomations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Automations\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\apis\\public\\api.py:57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mthread_local_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _thread_local_api_settings\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlaunch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LAUNCH_DEFAULT_PROJECT\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retry, runid\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecate\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\launch\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_launch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_and_run_agent, launch\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_launch_add\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m launch_add\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchAgent\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\launch\\_launch.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_project_spec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchProject\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchAgent\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\sdk\\launch\\loader.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Api\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_docker_installed\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msdk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlaunch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaunchError\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabstract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AbstractBuilder\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\wandb\\docker\\__init__.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple, Union\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdockerpycreds\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_executable  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auth, www_authenticate\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwandb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Error\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\dockerpycreds\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Store\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StoreError, CredentialsNotFound\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\dockerpycreds\\store.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m errors\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_environment_dict\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_executable\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStore\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Randy Cecil\\OneDrive\\Desktop\\A.I.-Assistant-1\\.venv\\Lib\\site-packages\\dockerpycreds\\utils.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspawn\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'distutils'"
          ]
        }
      ],
      "source": [
        "%pip install jax jaxlib flax optax orbax tiktoken wandb\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.601598Z",
          "iopub.status.busy": "2025-02-21T04:31:01.601360Z",
          "iopub.status.idle": "2025-02-21T04:31:01.605772Z",
          "shell.execute_reply": "2025-02-21T04:31:01.604615Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.601574Z"
        },
        "id": "xuMlCK3Q8WJD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "# Set mesh shape to match the number of available devices\n",
        "num_devices = len(jax.devices())\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((num_devices, 1)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.606937Z",
          "iopub.status.busy": "2025-02-21T04:31:01.606708Z",
          "iopub.status.idle": "2025-02-21T04:31:04.402839Z",
          "shell.execute_reply": "2025-02-21T04:31:04.401628Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.606915Z"
        },
        "id": "iWbkk1V7-Isg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.706850Z",
          "iopub.status.busy": "2025-02-21T04:32:00.706531Z",
          "iopub.status.idle": "2025-02-21T04:32:00.712524Z",
          "shell.execute_reply": "2025-02-21T04:32:00.711567Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.706823Z"
        },
        "id": "GRhiDsCrMZRp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 32  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 1024\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "Now define the model architecture. You can refer to [OpenAI's official implementation](https://github.com/openai/gpt-2) or [nanoGPT](https://github.com/karpathy/nanoGPT) for comparison. Main difference is with the sharding scheme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.771416Z",
          "iopub.status.busy": "2025-02-21T04:32:00.771150Z",
          "iopub.status.idle": "2025-02-21T04:32:00.792958Z",
          "shell.execute_reply": "2025-02-21T04:32:00.791974Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.771393Z"
        },
        "id": "z0p-IHurrB9i",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.mha = nnx.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            in_features=embed_dim,\n",
        "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.linear1 = nnx.Linear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=ff_dim,\n",
        "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.linear2 = nnx.Linear(\n",
        "            in_features=ff_dim,\n",
        "            out_features=embed_dim,\n",
        "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=param_dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "            break\n",
        "            generated.append(next_token)\n",
        "            # decode and print next_token\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBfT1dp5hMUm"
      },
      "source": [
        "Use Weights and Biases to track training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:25.482477Z",
          "iopub.status.busy": "2025-02-21T04:32:25.482072Z",
          "iopub.status.idle": "2025-02-21T04:32:28.629414Z",
          "shell.execute_reply": "2025-02-21T04:32:28.628576Z",
          "shell.execute_reply.started": "2025-02-21T04:32:25.482449Z"
        },
        "id": "IbhEtsganEWg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-pretraining',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "The goal is to have `train.bin` and `val.bin` in the same `data_dir`.\n",
        "\n",
        "* For Colab: place the files in Google Drive, at `/LLM-pretraining/OpenWebText/`\n",
        "* For Kaggle: The dataset is loaded by default upon notebook start, from https://www.kaggle.com/datasets/windmaple/openwebtext-gpt2\n",
        "* For GCP: The code checks for the presence of `OpenWebText-gpt2.zip` and assumes that it has been unzipped into the same directory as the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:28.630623Z",
          "iopub.status.busy": "2025-02-21T04:32:28.630426Z",
          "iopub.status.idle": "2025-02-21T04:32:28.647808Z",
          "shell.execute_reply": "2025-02-21T04:32:28.646734Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.630602Z"
        },
        "id": "rGUFsn1GMuzh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if platform == \"Colab\":\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  data_dir = \"/content/drive/MyDrive/LLM-pretraining/OpenWebText/\" # Assume dataset is already stored there\n",
        "elif platform == \"GCP\":\n",
        "  if not os.path.exists('OpenWebText-gpt2.zip'):\n",
        "    # Assume kaggle binary is in ~/.local/bin after pip install kaggle\n",
        "    !~/.local/bin/kaggle datasets download -d windmaple/OpenWebText-gpt2 && unzip OpenWebText-gpt2.zip\n",
        "  data_dir = \".\"\n",
        "elif platform == \"Kaggle\":  # On Kaggle one should manually add the datasets before running\n",
        "  data_dir = \"/kaggle/input/openwebtext-gpt2\"\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
        "def get_batch(train_or_eval = \"train\"):\n",
        "\n",
        "    data = train_data if train_or_eval == \"train\" else val_data\n",
        "\n",
        "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
        "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
        "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:28.648880Z",
          "iopub.status.busy": "2025-02-21T04:32:28.648690Z",
          "iopub.status.idle": "2025-02-21T04:32:28.683524Z",
          "shell.execute_reply": "2025-02-21T04:32:28.682396Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.648860Z"
        },
        "id": "8rRuTmABNV4b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRRpiYBpwr2R"
      },
      "source": [
        "Create the model and check the model parameter count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:28.684473Z",
          "iopub.status.busy": "2025-02-21T04:32:28.684258Z",
          "iopub.status.idle": "2025-02-21T04:32:33.129248Z",
          "shell.execute_reply": "2025-02-21T04:32:33.128221Z",
          "shell.execute_reply.started": "2025-02-21T04:32:28.684451Z"
        },
        "id": "D_L_PuTkwqtu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "\n",
        "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
        "import operator\n",
        "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBBJcz1B6XFS"
      },
      "source": [
        "Optionally check the parameter count against the same model on Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:33.130056Z",
          "iopub.status.busy": "2025-02-21T04:32:33.129850Z",
          "iopub.status.idle": "2025-02-21T04:32:33.134982Z",
          "shell.execute_reply": "2025-02-21T04:32:33.133692Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.130034Z"
        },
        "id": "KDWyHtrU6S5f",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# from transformers import GPT2Model\n",
        "\n",
        "# # From https://github.com/huggingface/transformers/issues/27615\n",
        "# model_hf = GPT2Model.from_pretrained('gpt2')\n",
        "# model_parameters = filter(lambda p: p.requires_grad, model_hf.parameters())\n",
        "# param_count = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "# print(f\"Number of model parameters from Hugging Face: {param_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:33.136140Z",
          "iopub.status.busy": "2025-02-21T04:32:33.135918Z",
          "iopub.status.idle": "2025-02-21T10:43:50.028572Z",
          "shell.execute_reply": "2025-02-21T10:43:50.027504Z",
          "shell.execute_reply.started": "2025-02-21T04:32:33.136118Z"
        },
        "id": "Ysl6CsfENeJN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "schedule = optax.cosine_decay_schedule(\n",
        "  init_value=init_learning_rate,\n",
        "  decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "train_metrics = nnx.metrics.Average('loss')\n",
        "val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"Initial generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'val_loss': []\n",
        "}\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    input_batch, target_batch = get_batch(\"train\")\n",
        "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
        "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      train_loss = float(train_metrics.compute())\n",
        "      metrics_history['train_loss'].append(train_loss)\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "      # eval step\n",
        "      input_val_batch, target_val_batch = get_batch('val')\n",
        "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), NamedSharding(mesh, P('batch', None))))\n",
        "      val_metrics.update(val_loss=loss, logits=logits)\n",
        "      val_loss = float(val_metrics.compute())\n",
        "      metrics_history['val_loss'].append(val_loss)\n",
        "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
        "      print(f\"Step {step + 1}, Validation loss: {val_loss}\")\n",
        "      train_metrics.reset()\n",
        "      val_metrics.reset()\n",
        "\n",
        "      start_time = time.time()\n",
        "    step += 1\n",
        "\n",
        "    if step > max_steps:\n",
        "      break\n",
        "\n",
        "# Final text generation\n",
        "print(f\"Final generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaLs6TD0lt5"
      },
      "source": [
        "Visualize the training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T10:43:50.030074Z",
          "iopub.status.busy": "2025-02-21T10:43:50.029672Z",
          "iopub.status.idle": "2025-02-21T10:43:51.584464Z",
          "shell.execute_reply": "2025-02-21T10:43:51.583724Z",
          "shell.execute_reply.started": "2025-02-21T10:43:50.030047Z"
        },
        "id": "B6Eg1Cz2y_iP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26_HtTkZTczd"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible sentences at the end of the training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hTE4MsEUh9K"
      },
      "source": [
        "Here are some training results on Kaggle TPU:\n",
        "\n",
        "| model | params | train loss | val loss | training time (TPU v3)\n",
        "| ------| ------ | ---------- | -------- | -----------------------\n",
        "| gpt2 | 124M         | 3.05  | 3.09     | 6.5 hr\n",
        "| gpt2-medium | 354M  | 2.83  | 2.86     | 22.5 hr\n",
        "\n",
        "The losses are roughly in line with [nanoGPT's](https://github.com/karpathy/nanoGPT?tab=readme-ov-file#baselines)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m13hUf7CcrSO"
      },
      "source": [
        "# Model saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T10:43:51.586015Z",
          "iopub.status.busy": "2025-02-21T10:43:51.585651Z",
          "iopub.status.idle": "2025-02-21T10:43:57.582123Z",
          "shell.execute_reply": "2025-02-21T10:43:57.581319Z",
          "shell.execute_reply.started": "2025-02-21T10:43:51.585991Z"
        },
        "id": "ofH_VkqMctZ5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "import shutil\n",
        "\n",
        "if platform == \"Colab\":\n",
        "  checkpoint_path = \"/content/checkpoints\"\n",
        "elif platform == \"Kaggle\":\n",
        "  checkpoint_path = \"/kaggle/working/checkpoints\"\n",
        "else:\n",
        "  from pathlib import Path\n",
        "  home = Path.home()\n",
        "  checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
        "\n",
        "# make sure the folder is empty and usable\n",
        "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "train_state = nnx.state(model)\n",
        "checkpointer.save(checkpoint_path, train_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soPqiR1JNmjf"
      },
      "source": [
        "# Model restoration\n",
        "Restore the model checkpoint. A pretrained checkpoint is also available [here](https://www.kaggle.com/models/windmaple/gpt2/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T10:43:57.583673Z",
          "iopub.status.busy": "2025-02-21T10:43:57.583415Z",
          "iopub.status.idle": "2025-02-21T10:44:12.704383Z",
          "shell.execute_reply": "2025-02-21T10:44:12.703322Z",
          "shell.execute_reply.started": "2025-02-21T10:43:57.583648Z"
        },
        "id": "EkoFGCgSZ1yz",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'orbax' has no attribute 'PyTreeCheckpointer'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model = nnx.eval_shape(\u001b[38;5;28;01mlambda\u001b[39;00m: create_model(rngs=nnx.Rngs(\u001b[32m0\u001b[39m)))\n\u001b[32m      2\u001b[39m state = nnx.state(model)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m checkpointer = \u001b[43morbax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTreeCheckpointer\u001b[49m()\n\u001b[32m      4\u001b[39m state = checkpointer.restore(checkpoint_path, item=state)\n\u001b[32m      5\u001b[39m nnx.update(model, state)\n",
            "\u001b[31mAttributeError\u001b[39m: module 'orbax' has no attribute 'PyTreeCheckpointer'"
          ]
        }
      ],
      "source": [
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.state(model)\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)\n",
        "\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Restored model generated text:\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "states: list = ['NY', 'NJ', 'CA', 'TX']\n",
            "tokenizer: Encoding = <Encoding 'gpt2'>\n",
            "top_k: int = 10\n",
            "vehicle_types: list = ['SUBN', 'SDN']\n",
            "violations: list = ['Double Parking', 'Expired Meter', 'No Parking', 'Fire Hydrant', 'Bus Stop']\n",
            "vocab_size: int = 50257\n",
            "weight_decay: float = 0.1\n"
          ]
        }
      ],
      "source": [
        "# The variables 'states', 'tokenizer', 'top_k', 'vehicle_types', 'violations', 'vocab_size', and 'weight_decay'\n",
        "# are already defined in previous cells with the correct values and types.\n",
        "# No action is needed; all are already fixed and available for use.\n",
        "for var_name, var_value in [\n",
        "    (\"states\", states),\n",
        "    (\"tokenizer\", tokenizer),\n",
        "    (\"top_k\", top_k),\n",
        "    (\"vehicle_types\", vehicle_types),\n",
        "    (\"violations\", violations),\n",
        "    (\"vocab_size\", vocab_size),\n",
        "    (\"weight_decay\", weight_decay)\n",
        "]:\n",
        "    print(f\"{var_name}: {type(var_value).__name__} = {var_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCApVd7671c1"
      },
      "source": [
        "# Disconnect the Colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T10:44:12.705749Z",
          "iopub.status.busy": "2025-02-21T10:44:12.705516Z",
          "iopub.status.idle": "2025-02-21T10:44:12.710493Z",
          "shell.execute_reply": "2025-02-21T10:44:12.709284Z",
          "shell.execute_reply.started": "2025-02-21T10:44:12.705726Z"
        },
        "id": "NsqYdbrDVKSq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "datasetId": 5848741,
          "sourceId": 10577797,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30920,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
